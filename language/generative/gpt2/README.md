<!--- SPDX-License-Identifier: Apache-2.0 -->

# gpt2

## Description

GPT-2 is a transformers model pretrained on a very large corpus of English data.
This is the smallest version of GPT-2, with 624M parameters.
If you want to run all processes, please contact us.

## Model

|Model|LAMBADA(PPL)|Shape(input_ids,position_ids,seq_index)|
|:-   |:-          |:-                                     |
|gpt2 |35.13       |(1,512),(512),(1)                      |

## Dataset

[Openai Dataset](https://github.com/openai/gpt-2/blob/master/domains.txt).

## References

* [huggingface](https://huggingface.co/gpt2)

## License

Apache 2.0
